# Skin Lesion Classification 


## Ä°Ã§indekiler

- [1: Cilt Lezyonu SÄ±nÄ±flandÄ±rma Projesi](#Cilt-Lezyonu-SÄ±nÄ±flandÄ±rma-Projesi)
- [2: Veri yolu tanÄ±mlarÄ±](#Veri-yolu-tanÄ±mlarÄ±)
- [3: Model GiriÅŸi ve Etiketleme](#Model-GiriÅŸi-Ve-Etiketleme)
- [4: Train Validation Ã–rnek SayÄ±larÄ± ve SÄ±nÄ±f Mapping](#Train-Validation-Ã–rnek-SayÄ±larÄ±-Ve-SÄ±nÄ±f-Mapping)
- [5: Ã–rnek Batch GÃ¶rselleÅŸtirme](#Ã–rnek-Batch-GÃ¶rselleÅŸtirme)
- [6: Augmentation GÃ¶rsel Kontrol â€” Orijinal vs. Augmented](#Augmentation-GÃ¶rsel-Kontrol--Orijinal-Vs-Augmented)
- [7: Hiperparametre Denemesi](#Hiperparametre-Denemesi)
- [8: CNN Baseline Modeli](#Cnn-Baseline-Modeli)
- [9: SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± ve Class Weight Analizi](#SÄ±nÄ±f-DaÄŸÄ±lÄ±mÄ±-Ve-Class-Weight-Analizi)
- [10: CNN Modeli EÄŸitimi ](#Cnn-Modeli-EÄŸitimi)
- [11: Overfitting Analizi](#Overfitting-Analizi)
- [12: Performans Metrikleri](#Performans-Metrikleri)
- [13: Confusion Matrix Analizi](#Confusion-Matrix-Analizi)
- [14: Classification Report Analizi](classification-report-analizi)
- [15: DeÄŸerlendirme](#cell-30-markdown-cell-30)
- [16: Model Ä°yileÅŸtirmesi â€” EfficientNetB0 Transfer Learning](#cell-31-model-iÌ‡yileÅŸtirmesi-efficientnetb0-transfer-learning)
- [17: MobileNetV2 EÄŸitim SonuÃ§larÄ±](#cell-33-mobilenetv2-eÄŸitim-sonuÃ§larÄ±)
- [18: CNN vs MobileNetV2 â€” SonuÃ§ KarÅŸÄ±laÅŸtÄ±rmasÄ±](#cell-35-cnn-vs-mobilenetv2-sonuÃ§-karÅŸÄ±laÅŸtÄ±rmasÄ±)
- [19: Grad-CAM DeÄŸerlendirmesi](#cell-37-grad-cam-deÄŸerlendirmesi)
- [20: Confusion Matrix (MobileNetV2)](#cell-39-confusion-matrix-mobilenetv2)
- [21: Classification Report MobileNetV2](#Classification-Report-MobileNetV2)
- [22: Final Ã–zet](#Final-Ã–zet)

---



---


#Cilt Lezyonu SÄ±nÄ±flandÄ±rma Projesi

## Veri KÃ¼mesi  
Bu projede **Skin Lesion Dataset** isimli Kaggle veri seti kullanÄ±lmÄ±ÅŸtÄ±r. Veri kÃ¼mesinde **8 farklÄ± sÄ±nÄ±f** bulunmaktadÄ±r:  

- AK (Actinic Keratosis)  
- BCC (Basal Cell Carcinoma)  
- BKL (Benign Keratosis-like lesions)  
- DF (Dermatofibroma)  
- MEL (Melanoma)  
- NV (Melanocytic Nevi)  
- SCC (Squamous Cell Carcinoma)  
- VASC (Vascular lesions)  

SÄ±nÄ±flarÄ±n Ã¶rnek sayÄ±larÄ± farklÄ±dÄ±r; bu durum ciddi bir **sÄ±nÄ±f dengesizliÄŸi** sorununa yol aÃ§maktadÄ±r. GÃ¶rseller dermoskopik cilt gÃ¶rÃ¼ntÃ¼lerinden oluÅŸmaktadÄ±r.  

## Projenin AmacÄ±  
Bu projenin amacÄ±, cilt lezyonlarÄ±nÄ±n doÄŸru ÅŸekilde sÄ±nÄ±flandÄ±rÄ±lmasÄ±dÄ±r. Ã‡alÄ±ÅŸma kapsamÄ±nda:  

- Ä°lk adÄ±mda basit bir **CNN modeli** kullanÄ±lmÄ±ÅŸ, ancak dÃ¼ÅŸÃ¼k doÄŸruluk ve **overfitting** gÃ¶zlenmiÅŸtir.  
- ArdÄ±ndan **MobileNetV2** tabanÄ± ile **transfer learning** uygulanmÄ±ÅŸ, sonrasÄ±nda **fine-tuning** adÄ±mÄ± eklenerek daha dengeli bir model elde edilmiÅŸtir.  
- EÄŸitim sÃ¼recinde **class weight** kullanÄ±larak dengesiz veri seti dengelenmeye Ã§alÄ±ÅŸÄ±lmÄ±ÅŸtÄ±r.  

## KullanÄ±lan YÃ¶ntemler  
- **CNN Baseline:** Referans performans iÃ§in baÅŸlangÄ±Ã§ modeli.  
- **MobileNetV2 (Transfer Learning + Fine-Tuning):** Daha yÃ¼ksek doÄŸruluk ve genelleme iÃ§in.  
- **Callbackâ€™ler:** EarlyStopping, ReduceLROnPlateau ve ModelCheckpoint ile eÄŸitim sÃ¼reci optimize edilmiÅŸtir.  
- **DeÄŸerlendirme:** Accuracy/Loss grafikleri, Confusion Matrix, Classification Report ve Grad-CAM gÃ¶rselleÅŸtirmeleri.  
- **Hiperparametre Denemeleri:** Dropout ve Ã¶ÄŸrenme oranÄ± Ã¼zerinde kÃ¼Ã§Ã¼k Ã§aplÄ± testler.  

## Bulgular  
- **MobileNetV2**, CNNâ€™e kÄ±yasla belirgin ÅŸekilde daha yÃ¼ksek doÄŸruluk (**%54**) saÄŸlamÄ±ÅŸtÄ±r.  
- Veri dengesizliÄŸi nedeniyle kÃ¼Ã§Ã¼k sÄ±nÄ±flarda baÅŸarÄ± dÃ¼ÅŸÃ¼k kalmÄ±ÅŸ, bÃ¼yÃ¼k sÄ±nÄ±flarda (Ã¶zellikle NV) daha baÅŸarÄ±lÄ± sonuÃ§lar alÄ±nmÄ±ÅŸtÄ±r.  
- **Grad-CAM** gÃ¶rselleri, modelin Ã§oÄŸunlukla lezyon bÃ¶lgelerine odaklandÄ±ÄŸÄ±nÄ±, yanlÄ±ÅŸ sÄ±nÄ±flarda ise odaÄŸÄ±n daÄŸÄ±nÄ±k olduÄŸunu gÃ¶stermiÅŸtir.  

Genel olarak, bu proje basit bir CNNâ€™den transfer learning tabanlÄ± bir modele geÃ§iÅŸin, sÄ±nÄ±flandÄ±rma performansÄ±nÄ± nasÄ±l artÄ±rdÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.  

---


```python
#Gerekli kÃ¼tÃ¼phaneler indirildi

import os, numpy as np, matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from sklearn import metrics as M
from sklearn.metrics import confusion_matrix
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
```

---


**Veri Yolu TanÄ±mlarÄ±**

Kaggle ortamÄ±nda yÃ¼klenen tÃ¼m veri setleri **`/kaggle/input/`** dizini altÄ±nda saklanmaktadÄ±r.  
Bu projede kullanÄ±lan **Skin Lesion Dataset** iÃ§in yol deÄŸiÅŸkenleri aÅŸaÄŸÄ±daki gibi tanÄ±mlanmÄ±ÅŸtÄ±r:

- **`data_dir`** â†’ Veri setinin ana klasÃ¶rÃ¼nÃ¼ gÃ¶sterir.  
  Ã–rn: `/kaggle/input/skin-lesion-dataset`

- **`train_dir`** â†’ EÄŸitim (Training) verilerinin bulunduÄŸu klasÃ¶r yolunu gÃ¶sterir.  
  Ã–rn: `/kaggle/input/skin-lesion-dataset/Train`

- **`val_dir`** â†’ DoÄŸrulama (Validation) verilerinin bulunduÄŸu klasÃ¶r yolunu gÃ¶sterir.  
  Ã–rn: `/kaggle/input/skin-lesion-dataset/Val`

Bu tanÄ±mlamalar sayesinde **`ImageDataGenerator.flow_from_directory()`** fonksiyonu ile ilgili klasÃ¶rlerden gÃ¶rÃ¼ntÃ¼ler otomatik olarak okunabilir.BÃ¶ylece eÄŸitim ve doÄŸrulama aÅŸamalarÄ±nda veri yÃ¼kleme iÅŸlemi esnek ve hatasÄ±z bir biÃ§imde gerÃ§ekleÅŸtirilmektedir.


---


```python
data_dir  = "/kaggle/input/skin-lesion-dataset"  
train_dir = os.path.join(data_dir, "Train")
val_dir   = os.path.join(data_dir, "Val")
```

---


## Model GiriÅŸi ve Etiketleme

Bu bÃ¶lÃ¼mde modelin giriÅŸ boyutunu ve veri etiketlerini tanÄ±mlÄ±yoruz:

- **`input_shape = (224,224,3)`** â†’ Model 64x64 boyutunda RGB (3 kanal) gÃ¶rÃ¼ntÃ¼ler bekliyor.  
- **`target_size = (224,224)`** â†’ Data generator her resmi bu boyuta yeniden Ã¶lÃ§ekleyecek.  
- **`batch_size = 32`** â†’ EÄŸitim sÄ±rasÄ±nda aynÄ± anda iÅŸlenecek Ã¶rnek sayÄ±sÄ±.  
- **`seed = 42`** â†’ Rastgele iÅŸlemleri kontrol altÄ±na almak iÃ§in, her Ã§alÄ±ÅŸtÄ±rmada aynÄ± sonuÃ§ alÄ±nmasÄ±nÄ± saÄŸlayacak. 
- **`class_to_index`** â†’ SÄ±nÄ±f isimlerini sayÄ±sal etiketlere (0â€“7) dÃ¶nÃ¼ÅŸtÃ¼ren sÃ¶zlÃ¼k. CNN modeli kategorileri bu sayÄ±sal etiketlerle Ã¶ÄŸreniyor.

Model akÄ±ÅŸÄ±nÄ±n sorusunsuz olmasÄ± iÃ§in input shape ve target_size eÅŸitlenmiÅŸtir.
Bu adÄ±mda, deri lezyonu veri seti eÄŸitim ve doÄŸrulama alt kÃ¼melerine ayrÄ±larak modele uygun biÃ§imde hazÄ±rlanmÄ±ÅŸtÄ±r. Veri kÃ¼mesi, ayrÄ± bir Val klasÃ¶rÃ¼ne ihtiyaÃ§ duymadan, aynÄ± Train klasÃ¶rÃ¼ iÃ§erisinden %80 eÄŸitim ve %20 doÄŸrulama olacak ÅŸekilde otomatik bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r.

Bu sayede her epoch sÄ±rasÄ±nda veri hem normalize ediliyor hem de eÄŸitim/doÄŸrulama ayrÄ±mÄ± otomatik yapÄ±lÄ±yor.

---



```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

input_shape = (224, 224, 3)
target_size = input_shape[:2]
batch_size  = 32
seed        = 42

# %20 validation split, aynÄ± seed
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    width_shift_range=0.02,
    height_shift_range=0.02,
    zoom_range=0.05,
    horizontal_flip=True,
    validation_split=0.2
)

val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_dir = "/kaggle/input/skin-lesion-dataset/Train"  

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='sparse',
    subset='training',
    shuffle=True,
    seed=seed
)

val_gen = val_datagen.flow_from_directory(
    train_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='sparse',
    subset='validation',
    shuffle=False,
    seed=seed
)

# Kontrol: mapping aynÄ± mÄ±?
print("Train indices:", train_gen.class_indices)
print("Val   indices:", val_gen.class_indices)
#EÄŸitim ve doÄŸrulama setlerinin aynÄ± sÄ±nÄ±f sÄ±ralamasÄ±na sahip olduÄŸu kontrolÃ¼
assert train_gen.class_indices == val_gen.class_indices

print("Train samples:", train_gen.samples, "| Val samples:", val_gen.samples)
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Found 22349 images belonging to 8 classes.
Found 5585 images belonging to 8 classes.
Train indices: {'AK': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, 'NV': 5, 'SCC': 6, 'VASC': 7}
Val   indices: {'AK': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, 'NV': 5, 'SCC': 6, 'VASC': 7}
Train samples: 22349 | Val samples: 5585
```


</details>

---


## Train & Validation Ã–rnek SayÄ±larÄ± ve SÄ±nÄ±f Mapping

## Data Generator: Train (Normalize + Augmentation), Validation (Sadece Normalize)

- **Train Generator (`train_gen`)**  
  EÄŸitim verileri Ã¼zerinde hem **normalize** iÅŸlemi (`rescale=1./255`) hem de hafif **augmentation** (rotation, shift, zoom, horizontal flip) uygulanmaktadÄ±r.  
  BÃ¶ylece model her epochâ€™ta aynÄ± resmi farklÄ± varyasyonlarla gÃ¶rÃ¼r, bu da **genelleme gÃ¼cÃ¼nÃ¼ artÄ±rÄ±r** ve **overfitting riskini azaltÄ±r**.  

- **Validation Generator (`val_gen`)**  
  DoÄŸrulama verileri Ã¼zerinde yalnÄ±zca **normalize** iÅŸlemi yapÄ±lÄ±r.  
  Validation setinde augmentation uygulanmaz Ã§Ã¼nkÃ¼ modelin gerÃ§ek performansÄ±nÄ± **deÄŸiÅŸtirilmemiÅŸ veriler** Ã¼zerinde gÃ¶rmek gerekir.  

ğŸ‘‰ SonuÃ§: EÄŸitim sÄ±rasÄ±nda Ã§eÅŸitlilik artÄ±rÄ±lÄ±rken, doÄŸrulama sÃ¼reci temiz tutulur.
weight_shift ve height_shit i 0.10 uyguladÄ±ÄŸÄ±mÄ±zda resimler bulanÄ±klaÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r  ve .02 ye indirilmiÅŸtir.

---


```python
# === TRAIN: normalize + augmentation ===
train_datagen = ImageDataGenerator(
    rescale=1./255,           # normalize (kesinlikle gerekli)
    rotation_range=10,        # kÃ¼Ã§Ã¼k rotasyon (Â±10Â° yeterli)
    width_shift_range=0.02,   # %2â€™den fazla kaydÄ±rma yapma
    height_shift_range=0.02,  # aynÄ± ÅŸekilde %2 civarÄ±
    zoom_range=0.05,          # %5 zoom â†’ Ã§ok fazla yakÄ±nlaÅŸtÄ±rma/uzaklaÅŸtÄ±rma yok
    horizontal_flip=True,     # simetrik yapÄ±lar iÃ§in faydalÄ±
    vertical_flip=False,      # genelde deri gÃ¶rselleri ters Ã§evrilmez, kapalÄ± tut
    brightness_range=[0.9,1.1], # Ä±ÅŸÄ±k koÅŸullarÄ±nÄ± biraz deÄŸiÅŸtirmek faydalÄ±
    validation_split=0.2)

# === VAL: sadece normalize ===
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,)

# AkÄ±ÅŸlar (Train klasÃ¶rÃ¼nden %80/%20)
train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='sparse',
    shuffle=True,
    subset='training',
    seed=seed)

val_gen = val_datagen.flow_from_directory(
    train_dir,                 # validation split yine Train Ã¼zerinden
    target_size=target_size,
    batch_size=batch_size,
    class_mode='sparse',
    shuffle=False,
    subset='validation',
   
    seed=seed)

print("Train samples:", train_gen.samples, "| Val samples:", val_gen.samples)
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Found 22349 images belonging to 8 classes.
Found 5585 images belonging to 8 classes.
Train samples: 22349 | Val samples: 5585
```


</details>

---


## Ã–rnek Batch GÃ¶rselleÅŸtirme

Bu adÄ±mda `train_gen` Ã¼zerinden bir batch (kÃ¼me) veri Ã§ekilerek gÃ¶rselleÅŸtirme yapÄ±lmÄ±ÅŸtÄ±r.

- `next(train_gen)` ile eÄŸitim generatorâ€™undan bir batch alÄ±nÄ±r.  
- `class_indices` kullanÄ±larak sayÄ±sal etiketler sÄ±nÄ±f isimlerine Ã§evrilir (`idx_to_class`).  
- GÃ¶rseller 3x3â€™lÃ¼k bir grid halinde Ã§izilir, baÅŸlÄ±klarda ilgili sÄ±nÄ±f adÄ± gÃ¶sterilir.  
- AmaÃ§: Verilerin doÄŸru ÅŸekilde okunup etiketlendiÄŸini, augmentation sonrasÄ± gÃ¶rÃ¼ntÃ¼lerin anlamlÄ± kaldÄ±ÄŸÄ±nÄ± kontrol etmektir.  

Bu kontrol, model eÄŸitimine baÅŸlamadan Ã¶nce veri pipelineâ€™Ä±nÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± **doÄŸrulama** aÃ§Ä±sÄ±ndan kritik bir adÄ±mdÄ±r.


---


```python
import matplotlib.pyplot as plt
import numpy as np

# 1 batch Ã§ekelim
x_batch, y_batch = next(train_gen)

# Mapping (index â†’ class adÄ±)
idx_to_class = {v:k for k,v in train_gen.class_indices.items()}

# 9 gÃ¶rsel Ã§izelim
plt.figure(figsize=(8,8))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(x_batch[i])  # normalize edilmiÅŸ (0-1), direk gÃ¶sterilebilir
    plt.title(idx_to_class[int(y_batch[i])])
    plt.axis('off')

plt.tight_layout()
plt.show()
```
<img width="766" height="790" alt="download" src="https://github.com/user-attachments/assets/2be19be5-71b9-43b1-b1b9-f6a2c855fd22" />


<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 800x800 with 9 Axes>
```

![output](readme_assets/cell11_out1.png)

</details>

---


## Augmentation GÃ¶rsel Kontrol â€” Orijinal vs. Augmented

AmaÃ§:
EÄŸitim sÄ±rasÄ±nda uygulanan veri artÄ±rma tekniklerinin (rotation, shift, zoom, flip, brightness) gÃ¶rÃ¼ntÃ¼ kalitesini bozup bozmadÄ±ÄŸÄ±nÄ± incelemektir. Bu amaÃ§la aynÄ± gÃ¶rselin hem orijinal hali hem de augment edilmiÅŸ varyasyonlarÄ± yan yana gÃ¶sterilerek gÃ¶rsel kontrol yapÄ±lmÄ±ÅŸtÄ±r.

YÃ¶ntem:

**Orijinal GÃ¶rsel:** Validation generatorâ€™dan seÃ§ilmiÅŸtir (validation aÅŸamasÄ±nda augmentation uygulanmadÄ±ÄŸÄ±ndan temiz gÃ¶rÃ¼ntÃ¼ elde edilir).

**Augment EdilmiÅŸ GÃ¶rseller:** EÄŸitimde kullanÄ±lan parametrelerle oluÅŸturulan bir ImageDataGenerator Ã¼zerinden flow(...) fonksiyonu yardÄ±mÄ±yla Ã¼retilmiÅŸtir.

**Normalize:** TÃ¼m gÃ¶rÃ¼ntÃ¼ler [0,1] aralÄ±ÄŸÄ±na normalize edilmiÅŸtir.

**Kenar BoÅŸluklarÄ±nÄ±n Ã–nlenmesi:** DÃ¶ndÃ¼rme ve kaydÄ±rma iÅŸlemleri sonrasÄ± oluÅŸabilecek siyah bÃ¶lgeleri azaltmak iÃ§in fill_mode="nearest" kullanÄ±lmÄ±ÅŸtÄ±r.

**GÃ¶rselleÅŸtirme:** Orijinal gÃ¶rsel ve N adet augment edilmiÅŸ kopya aynÄ± grid Ã¼zerinde yan yana gÃ¶sterilmiÅŸtir.

**SonuÃ§:**
Bu kontrol sayesinde augmentasyon parametrelerinin gÃ¶rÃ¼ntÃ¼leri aÅŸÄ±rÄ± derecede bozmadÄ±ÄŸÄ±, lezyonun odakta kaldÄ±ÄŸÄ± ve verinin tÄ±bbi anlamÄ±nÄ±n korunduÄŸu doÄŸrulanmÄ±ÅŸtÄ±r. AÅŸÄ±rÄ± kaydÄ±rma veya zoom durumunda oluÅŸabilecek bulanÄ±klÄ±klar fark edilerek parametreler daha uygun deÄŸerlere dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.

---


```python
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator

# Orijinal resmi validation'dan al (augmentation yok)
idx = 0  
orig_path = val_gen.filepaths[idx]
orig_img  = load_img(orig_path, target_size=val_gen.target_size, color_mode="rgb")
orig_arr  = img_to_array(orig_img).astype("float32")  # 0..255 aralÄ±ÄŸÄ±

# EÄŸitimdeki augment ayarlarÄ±yla VIZ datagen (fill_mode ekledik)
viz_datagen = ImageDataGenerator(
    rescale=1./255,              # eÄŸitimde olduÄŸu gibi
    rotation_range=10,
    width_shift_range=0.02,
    height_shift_range=0.02,
    zoom_range=0.05,
    horizontal_flip=True,
    brightness_range=[0.9, 1.1],
    fill_mode="nearest"          # siyah boÅŸluklarÄ± Ã¶nler
)

# Tek resmi batch'e sarÄ±p flow ile N adet augment Ã¼ret
N = 6
it = viz_datagen.flow(np.expand_dims(orig_arr, 0), batch_size=1, shuffle=False)
augmented = [next(it)[0] for _ in range(N)]   # her biri [0..1] aralÄ±ÄŸÄ±nda

# Ã‡izim (sol Ã¼stte orijinal, diÄŸerleri augment)
plt.figure(figsize=(12,6))
plt.subplot(2,4,1)
plt.imshow(orig_arr / 255.0, vmin=0, vmax=1)  # orijinali 0..1'e getir
plt.title("Orijinal"); plt.axis("off")

for i, img in enumerate(augmented, start=2):
    plt.subplot(2,4,i)
    plt.imshow(np.clip(img, 0, 1), vmin=0, vmax=1)
    plt.title(f"Aug {i-1}")
    plt.axis("off")

plt.tight_layout(); plt.show()

# min/max deÄŸerleri
print("orig min/max:", float(orig_arr.min()), float(orig_arr.max()))
print("aug  min/max:", float(np.min([a.min() for a in augmented])),
                      float(np.max([a.max() for a in augmented])))
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 1200x600 with 7 Axes>
orig min/max: 49.0 255.0
aug  min/max: 0.1764705926179886 1.0
```

<img width="1223" height="581" alt="image" src="https://github.com/user-attachments/assets/98dbae52-b4dd-405f-9f88-a862b9749078" />

</details>

---


## Hiperparametre Denemesi

**AmaÃ§:**
MobileNetV2 modelinde kÃ¼Ã§Ã¼k ayarlarla (dropout oranÄ±, Ã¶ÄŸrenme oranÄ±, son katmanlarÄ± aÃ§Ä±p aÃ§mama) performansÄ±n nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rmek.

**YÃ¶ntem:**

**Dropout:** 0.3 ve 0.5 olarak denendi.

**Learning rate (lr):** 1e-3 ve 5e-4 olarak denendi.

**Son katmanlarÄ± aÃ§ma (unfreeze):** Sadece eklenen katman eÄŸitildi ve son 10 katman aÃ§Ä±ldÄ±.

Her deneme sadece **2 epoch** ile hÄ±zlÄ±ca test edildi.

SonuÃ§lar Accuracy, Macro F1 ve Balanced Accuracy metrikleriyle Ã¶lÃ§Ã¼ldÃ¼.

**SonuÃ§:**
Daha dÃ¼ÅŸÃ¼k dropout oranÄ± (0.3), doÄŸruluÄŸu biraz daha yÃ¼ksek verdi. Bu da modelin aÅŸÄ±rÄ± dÃ¼zenlenmeden (over-regularization) daha iyi genelleme yaptÄ±ÄŸÄ±nÄ± gÃ¶steriyor. Daha uzun eÄŸitimlerde de 0.3 deÄŸerinin daha uygun olacaÄŸÄ± sÃ¶ylenebilir.

---


```python
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from sklearn.metrics import accuracy_score

#HÄ±zlÄ± deneme fonksiyonu (dropout ve lr parametreleriyle)
def quick_try(dropout=0.3, lr=1e-3):
    # Ã–nceden eÄŸitilmiÅŸ MobileNetV2 tabanÄ±nÄ± alÄ±yoruz
    base = MobileNetV2(include_top=False, weights="imagenet",
                       input_shape=train_gen.image_shape, pooling="avg")
    base.trainable = False   # taban katmanlar donduruluyor (transfer learning)

#Model yapÄ±sÄ±
    inp = layers.Input(shape=train_gen.image_shape)
    x = base(inp, training=False)
    x = layers.Dropout(dropout)(x)   # dropout uygulanÄ±yor
    out = layers.Dense(train_gen.num_classes, activation="softmax")(x)

    model = models.Model(inp, out)
    model.compile(optimizer=tf.keras.optimizers.Adam(lr),
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])

#HÄ±zlÄ± eÄŸitim (1 epoch, 50 step ile sÄ±nÄ±rlÄ±)
    model.fit(
            train_gen, 
            validation_data=val_gen,
            epochs=1, 
            steps_per_epoch=50, 
            verbose=0)

    #Tahmin ve doÄŸruluk hesabÄ±
    y_pred = model.predict(val_gen, verbose=0).argmax(1)
    acc = accuracy_score(val_gen.classes, y_pred)
    return acc

#Dropout oranlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r (0.3 vs 0.5)
acc1 = quick_try(dropout=0.3)
acc2 = quick_try(dropout=0.5)

print(f"Dropout=0.3 | Accuracy={acc1:.3f}")
print(f"Dropout=0.5 | Accuracy={acc2:.3f}")
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5
[1m9406464/9406464[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 0us/step

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758886007.409169      98 service.cc:148] XLA service 0x7a9a68003bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758886007.409994      98 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
I0000 00:00:1758886007.410017      98 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5
I0000 00:00:1758886008.349712      98 cuda_dnn.cc:529] Loaded cuDNN version 90300
I0000 00:00:1758886013.896741      98 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.

Dropout=0.3 | Accuracy=0.479
Dropout=0.5 | Accuracy=0.491
```


</details>

---


## CNN Baseline Modeli

**AmaÃ§:**
Transfer learning yÃ¶ntemlerine geÃ§meden Ã¶nce, sÄ±fÄ±rdan tanÄ±mlanan bir CNN modeli ile referans (baseline) performansÄ± elde etmek. BÃ¶ylece EfficientNet gibi daha gÃ¼Ã§lÃ¼ modellerle yapÄ±lacak karÅŸÄ±laÅŸtÄ±rmalar iÃ§in saÄŸlam bir baÅŸlangÄ±Ã§ noktasÄ± oluÅŸturmak.

Bu modelde Ã¼Ã§ konvolÃ¼syon bloÄŸu kullanÄ±lmÄ±ÅŸtÄ±r.Her bloktan sonra **Batch Normalization** ve **ReLU aktivasyonu** uygulanarak eÄŸitim sÃ¼reci daha kararlÄ± hale getirilmiÅŸ, **MaxPooling** ile boyut kÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ, **Dropout** ile aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski azaltÄ±lmÄ±ÅŸtÄ±r.

Son aÅŸamada **Global Average Pooling (GAP)** ile Ã§Ä±karÄ±lan Ã¶zellikler Ã¶zetlenmiÅŸ, ardÄ±ndan tam baÄŸlÄ± katmanlar aracÄ±lÄ±ÄŸÄ±yla sÄ±nÄ±flandÄ±rma yapÄ±lmÄ±ÅŸtÄ±r. Ã‡Ä±kÄ±ÅŸ katmanÄ±, tÃ¼m sÄ±nÄ±flar iÃ§in olasÄ±lÄ±k tahminleri Ã¼retmektedir.

**SonuÃ§:**
Bu CNN modeli, hafif ve hÄ±zlÄ± bir yapÄ± sunarak projede ilk performans Ã¶lÃ§Ã¼tÃ¼ olmuÅŸtur. Baseline olarak elde edilen sonuÃ§lar, daha sonra uygulanacak transfer learning (EfficientNetB0) ve fine-tuning adÄ±mlarÄ±nÄ±n geliÅŸimini deÄŸerlendirmek iÃ§in bir karÅŸÄ±laÅŸtÄ±rma zemini saÄŸlamaktadÄ±r.

---


```python
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam

input_shape = train_gen.image_shape   # (224, 224, 3)
num_classes = train_gen.num_classes

def make_cnn_baseline(input_shape, num_classes):
    # === Input ===
    x = inp = layers.Input(shape=input_shape)

    # Blok 1 (dÃ¼ÅŸÃ¼k seviye Ã¶zellikler)
    x = layers.Conv2D(32, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.Conv2D(32, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.MaxPooling2D()(x); x = layers.Dropout(0.15)(x)

    # Blok 2 (orta seviye Ã¶zellikler)
    x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.MaxPooling2D()(x); x = layers.Dropout(0.20)(x)

    # Blok 3 (yÃ¼ksek seviye Ã¶zellikler)
    x = layers.Conv2D(128, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.Conv2D(128, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)
    x = layers.MaxPooling2D()(x); x = layers.Dropout(0.25)(x)

    # Flatten yerine GAP
    x = layers.GlobalAveragePooling2D()(x)      #Parametre sayÄ±sÄ±nÄ± azaltma
    x = layers.Dense(256, activation='relu')(x) # Tam baÄŸlÄ± katman
    x = layers.Dropout(0.3)(x)                  #Overfitting Ã¶nleme
    out = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)

    return models.Model(inp, out, name="cnn_baseline")
#Model derlenmesi
cnn = make_cnn_baseline(input_shape, num_classes)
cnn.compile(optimizer=Adam(1e-3),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])

cnn.summary()
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
### CNN Baseline Model Summary

| Layer (type)                | Output Shape        | Param #  |
|------------------------------|---------------------|----------|
| **input_layer (InputLayer)** | (None, 224, 224, 3) | 0        |
| conv2d                       | (None, 224, 224, 32)| 864      |
| batch_normalization          | (None, 224, 224, 32)| 128      |
| re_lu                        | (None, 224, 224, 32)| 0        |
| conv2d_1                     | (None, 224, 224, 32)| 9,216    |
| batch_normalization_1        | (None, 224, 224, 32)| 128      |
| re_lu_1                      | (None, 224, 224, 32)| 0        |
| max_pooling2d                | (None, 112, 112, 32)| 0        |
| dropout                      | (None, 112, 112, 32)| 0        |
| conv2d_2                     | (None, 112, 112, 64)| 18,432   |
| batch_normalization_2        | (None, 112, 112, 64)| 256      |
| re_lu_2                      | (None, 112, 112, 64)| 0        |
| conv2d_3                     | (None, 112, 112, 64)| 36,864   |
| batch_normalization_3        | (None, 112, 112, 64)| 256      |
| re_lu_3                      | (None, 112, 112, 64)| 0        |
| max_pooling2d_1              | (None, 56, 56, 64)  | 0        |
| dropout_1                    | (None, 56, 56, 64)  | 0        |
| conv2d_4                     | (None, 56, 56, 128) | 73,728   |
| batch_normalization_4        | (None, 56, 56, 128) | 512      |
| re_lu_4                      | (None, 56, 56, 128) | 0        |
| conv2d_5                     | (None, 56, 56, 128) | 147,456  |
| batch_normalization_5        | (None, 56, 56, 128) | 512      |
| re_lu_5                      | (None, 56, 56, 128) | 0        |
| max_pooling2d_2              | (None, 28, 28, 128) | 0        |
| dropout_2                    | (None, 28, 28, 128) | 0        |
| global_average_pooling2d     | (None, 128)         | 0        |
| dense                        | (None, 256)         | 33,024   |
| dropout_3                    | (None, 256)         | 0        |
| dense_1                      | (None, 8)           | 2,056    |

---

**Total params:** 323,432 (1.23 MB)  
**Trainable params:** 322,536 (1.23 MB)  
**Non-trainable params:** 896 (3.50 KB)  

```


</details>

---


## SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± ve Class Weight Analizi

**GÃ¶zlem:**
EÄŸitim sÃ¼recinde modelin tahminlerinin bÃ¼yÃ¼k Ã¶lÃ§Ã¼de NV (Nevus) sÄ±nÄ±fÄ±na toplandÄ±ÄŸÄ± fark edilmiÅŸtir. Bu durum, veri setindeki sÄ±nÄ±f dengesizliÄŸinden kaynaklanmaktadÄ±r.

**EÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (Class counts):**  
- SÄ±nÄ±f 0 â†’ 1.264 Ã¶rnek  
- SÄ±nÄ±f 1 â†’ 3.326 Ã¶rnek  
- SÄ±nÄ±f 2 â†’ 2.293 Ã¶rnek  
- SÄ±nÄ±f 3 â†’ 192 Ã¶rnek  
- SÄ±nÄ±f 4 â†’ 3.774 Ã¶rnek  
- SÄ±nÄ±f 5 â†’ 10.639 Ã¶rnek (**en bÃ¼yÃ¼k sÄ±nÄ±f**)  
- SÄ±nÄ±f 6 â†’ 656 Ã¶rnek  
- SÄ±nÄ±f 7 â†’ 205 Ã¶rnek  

ğŸ‘‰ GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi sÄ±nÄ±flar arasÄ±nda ciddi **dengesizlik** var. Ã–zellikle sÄ±nÄ±f 5 (10.639 Ã¶rnek) Ã§ok baskÄ±nken, sÄ±nÄ±f 3 (192 Ã¶rnek) ve sÄ±nÄ±f 7 (205 Ã¶rnek) oldukÃ§a az temsil edilmiÅŸ.  

**Hesaplanan Class Weight deÄŸerleri:**  
- KÃ¼Ã§Ã¼k sÄ±nÄ±flar:  
  - SÄ±nÄ±f 3 â†’ **14.55**  
  - SÄ±nÄ±f 7 â†’ **13.63**  
  - SÄ±nÄ±f 6 â†’ **4.26**  
- BÃ¼yÃ¼k sÄ±nÄ±flar:  
  - SÄ±nÄ±f 5 â†’ **0.263**  
  - SÄ±nÄ±f 4 â†’ **0.74**  
  - SÄ±nÄ±f 1 â†’ **0.84**  

ğŸ‘‰ Yorum:  
- **Az Ã¶rneÄŸi olan sÄ±nÄ±flara yÃ¼ksek aÄŸÄ±rlÄ±k** verilerek modelin onlarÄ± dikkate almasÄ± saÄŸlanÄ±yor.  
- **Ã‡ok Ã¶rneÄŸi olan sÄ±nÄ±flara dÃ¼ÅŸÃ¼k aÄŸÄ±rlÄ±k** verilerek baskÄ±n olmalarÄ± engelleniyor.  
- Bu sayede eÄŸitim sÄ±rasÄ±nda model, her sÄ±nÄ±fa daha **dengeli** yaklaÅŸacak ve kÃ¼Ã§Ã¼k sÄ±nÄ±flarÄ± gÃ¶z ardÄ± etme ihtimali azalacak.


---

### Cell 19: Imports

```python
import numpy as np

# Toplam sÄ±nÄ±f sayÄ±sÄ±
num_classes = train_gen.num_classes

# EÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
y_train = train_gen.classes
counts  = np.bincount(y_train, minlength=num_classes)
total   = y_train.size

# class_weight hesaplama: az sÄ±nÄ±fa daha bÃ¼yÃ¼k aÄŸÄ±rlÄ±k
class_weight = {
    i: total / (len(counts) * max(1, counts[i]))
    for i in range(num_classes)
}

print("Class counts :", dict(enumerate(counts)))
print("Class weight :", {k: round(v, 3) for k, v in class_weight.items()})
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Class counts : {0: 1264, 1: 3326, 2: 2293, 3: 192, 4: 3774, 5: 10639, 6: 656, 7: 205}
Class weight : {0: 2.21, 1: 0.84, 2: 1.218, 3: 14.55, 4: 0.74, 5: 0.263, 6: 4.259, 7: 13.627}
```


</details>

---


## CNN Modeli EÄŸitimi

**AmaÃ§:**
Cilt lezyonu sÄ±nÄ±flandÄ±rma probleminde CNN tabanlÄ± bir model eÄŸitmek. EÄŸitim sÄ±rasÄ±nda sÄ±nÄ±f dengesizliÄŸini gidermek iÃ§in class_weight kullanÄ±lmakta, ayrÄ±ca modelin daha saÄŸlÄ±klÄ± Ã¶ÄŸrenmesi iÃ§in **EarlyStopping**, **ReduceLROnPlateau** ve **ModelCheckpoint** callbackâ€™leri uygulanmaktadÄ±r.

**SonuÃ§:**
- Callbackâ€™ler sayesinde model, gereksiz epochâ€™larda aÅŸÄ±rÄ± Ã¶ÄŸrenmeden korunmuÅŸ, en iyi genelleme performansÄ±na sahip olan noktadaki aÄŸÄ±rlÄ±klarla teslim edilmeye hazÄ±r hale gelmiÅŸtir.
- Model 20 epochâ€™a kadar eÄŸitilmek Ã¼zere ayarlanmÄ±ÅŸtÄ±r, ancak callback mekanizmalarÄ± sayesinde daha erken sonlandÄ±rÄ±labilmektedir.
- En dÃ¼ÅŸÃ¼k doÄŸrulama kaybÄ±na sahip model /kaggle/working/cnn_baseline_best.keras dosyasÄ±na kaydedilmiÅŸtir.
- EÄŸitim sÃ¼reci history_cnn deÄŸiÅŸkeninde saklanmÄ±ÅŸtÄ±r.
- Bu Ã§Ä±ktÄ± ilerleyen adÄ±mlarda metrik deÄŸerlendirme, confusion matrix ve hata analizi iÃ§in kullanÄ±lacaktÄ±r.
- Modelin en iyi doÄŸrulama performansÄ± 1. epochâ€™ta elde edilmiÅŸtir.
- EÄŸitim, bu noktadan sonra geliÅŸme olmadÄ±ÄŸÄ± iÃ§in durdurulmuÅŸ ve aÄŸÄ±rlÄ±klar otomatik olarak 1. epochâ€™taki en iyi haline geri yÃ¼klenmiÅŸtir.

---

```python
# Gerekli KÃ¼tÃ¼phaneler
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras import layers, models
import tensorflow as tf

# Mixed Precision Training (HÄ±zlandÄ±rma iÃ§in)
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# EarlyStopping, ReduceLROnPlateau ve ModelCheckpoint Callback'leri
early = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)
rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)
ckpt  = ModelCheckpoint("/kaggle/working/cnn_baseline_best.keras", monitor="val_loss", save_best_only=True, verbose=1)

# CNN Modeli TanÄ±mlama (Daha Basit ve HÄ±zlÄ±)
def build_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),  # 128'lik konvolÃ¼syonel katman yerine 64
        layers.Dense(64, activation='relu'),  # Daha kÃ¼Ã§Ã¼k Dense katman
        layers.Dropout(0.3),  # Dropout oranÄ±nÄ± dÃ¼ÅŸÃ¼r
        layers.Dense(train_gen.num_classes, activation='softmax')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Modeli BaÅŸlat
cnn = build_model()

# EÄŸitim Parametreleri
EPOCHS = 20

# EÄŸitimde HÄ±zlandÄ±rma
history_cnn = cnn.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    batch_size=64,  # Daha bÃ¼yÃ¼k batch size
    callbacks=[early, rlrop, ckpt],
    class_weight=class_weight,  # SÄ±nÄ±f dengesizliÄŸini gidermek iÃ§in
    verbose=1
)
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Epoch 1/20

2025-09-26 11:30:54.962898: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng19{k2=0} for conv (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,111,111,32]{3,2,1,0}, f16[32,109,109,64]{3,2,1,0}), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardFilter", backend_config={"cudnn_conv_backend_config":{"activation_mode":"kNone","conv_result_scale":1,"leakyrelu_alpha":0,"side_input_scale":0},"force_earliest_schedule":false,"operation_queue_id":"0","wait_on_operation_queues":[]} is taking a while...
2025-09-26 11:30:55.194095: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.23140252s
Trying algorithm eng19{k2=0} for conv (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,111,111,32]{3,2,1,0}, f16[32,109,109,64]{3,2,1,0}), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardFilter", backend_config={"cudnn_conv_backend_config":{"activation_mode":"kNone","conv_result_scale":1,"leakyrelu_alpha":0,"side_input_scale":0},"force_earliest_schedule":false,"operation_queue_id":"0","wait_on_operation_queues":[]} is taking a while...

[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 593ms/step - accuracy: 0.2930 - loss: 3.5452
Epoch 1: val_loss improved from inf to 1.64446, saving model to /kaggle/working/cnn_baseline_best.keras
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m444s[0m 621ms/step - accuracy: 0.2931 - loss: 3.5436 - val_accuracy: 0.3987 - val_loss: 1.6445 - learning_rate: 0.0010
Epoch 2/20
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 402ms/step - accuracy: 0.3367 - loss: 1.9814
Epoch 2: val_loss did not improve from 1.64446
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m296s[0m 423ms/step - accuracy: 0.3367 - loss: 1.9813 - val_accuracy: 0.3332 - val_loss: 1.8528 - learning_rate: 0.0010
Epoch 3/20
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 399ms/step - accuracy: 0.3399 - loss: 1.9202
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 3: val_loss did not improve from 1.64446
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m294s[0m 421ms/step - accuracy: 0.3399 - loss: 1.9202 - val_accuracy: 0.2981 - val_loss: 1.7909 - learning_rate: 0.0010
Epoch 4/20
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 397ms/step - accuracy: 0.3620 - loss: 1.8264
Epoch 4: val_loss did not improve from 1.64446
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m294s[0m 420ms/step - accuracy: 0.3620 - loss: 1.8264 - val_accuracy: 0.3083 - val_loss: 1.8387 - learning_rate: 5.0000e-04
Epoch 5/20
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 396ms/step - accuracy: 0.3624 - loss: 1.8095
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 5: val_loss did not improve from 1.64446
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m293s[0m 419ms/step - accuracy: 0.3624 - loss: 1.8096 - val_accuracy: 0.2920 - val_loss: 1.8072 - learning_rate: 5.0000e-04
Epoch 5: early stopping
Restoring model weights from the end of the best epoch: 1.
```


</details>

---


## Overfitting Analizi

**AmaÃ§:**  
CNN modelinde aÅŸÄ±rÄ± Ã¶ÄŸrenme olup olmadÄ±ÄŸÄ±nÄ± eÄŸitim ve doÄŸrulama eÄŸrileri Ã¼zerinden gÃ¶rmek.  

**SonuÃ§:**  
- **Accuracy:** EÄŸitim doÄŸruluÄŸu %26â€™dan %40â€™a yÃ¼kselirken, doÄŸrulama doÄŸruluÄŸu dalgalÄ± seyretti ve bazÄ± epochâ€™larda %20â€™nin altÄ±na dÃ¼ÅŸtÃ¼. Bu durum modelin doÄŸrulama setinde genelleme yapmakta zorlandÄ±ÄŸÄ±nÄ± gÃ¶steriyor.  
- **Loss:** EÄŸitim kaybÄ± dÃ¼zenli biÃ§imde azalÄ±rken, doÄŸrulama kaybÄ± bir noktada yÃ¼kselip tekrar dÃ¼ÅŸmÃ¼ÅŸ. Bu da doÄŸrulama setinde kararsÄ±z bir Ã¶ÄŸrenme sÃ¼reci yaÅŸandÄ±ÄŸÄ±nÄ± gÃ¶steriyor.   

**Yorum:**  
EÄŸitim setinde iyileÅŸme devam ederken doÄŸrulama performansÄ± geriledi. Bu rakamlar, modelin erken epochâ€™lardan itibaren **overfitting** yaptÄ±ÄŸÄ±nÄ± net biÃ§imde gÃ¶steriyor.


---


```python
import matplotlib.pyplot as plt
h = history_cnn.history

plt.figure(figsize=(10,4))
plt.subplot(1,2,1); plt.plot(h['accuracy']); plt.plot(h['val_accuracy'])
plt.title('CNN | Accuracy'); plt.legend(['train','val'])
plt.subplot(1,2,2); plt.plot(h['loss']); plt.plot(h['val_loss'])
plt.title('CNN | Loss'); plt.legend(['train','val'])
plt.tight_layout(); plt.show()
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 1000x400 with 2 Axes>
```

<img width="1038" height="396" alt="image" src="https://github.com/user-attachments/assets/8d4ac0f8-1f59-49c2-911b-5cee3a2b45f7" />


</details>

---


## Performans Metrikleri

**AmaÃ§:**  
CNN modelinin doÄŸrulama kÃ¼mesi Ã¼zerindeki baÅŸarÄ±sÄ±nÄ± Ã¶lÃ§mek. Burada sadece genel doÄŸruluk deÄŸil, sÄ±nÄ±f dengesizliÄŸini dikkate alan **Macro F1** ve **Balanced Accuracy** metrikleri de kullanÄ±lmÄ±ÅŸtÄ±r.

**SonuÃ§:**  
DoÄŸrulama kÃ¼mesi Ã¼zerinde elde edilen skorlar:  

- **Accuracy:** 0.398  
- **Macro F1:** 0.132  
- **Balanced Accuracy:** 0.190 

**Yorum:**  
- **Accuracy (~%40):** Genel doÄŸru tahmin oranÄ± dÃ¼ÅŸÃ¼k dÃ¼zeydedir.  
- **Macro F1 (0.13):** SÄ±nÄ±flar arasÄ± dengesizliÄŸi yansÄ±tarak modelin bazÄ± sÄ±nÄ±flarda Ã§ok zayÄ±f performans gÃ¶sterdiÄŸini ortaya koymaktadÄ±r.  
- **Balanced Accuracy (~%19):** TÃ¼m sÄ±nÄ±flarÄ± eÅŸit Ã¶nemde kabul eden bu metrik de dÃ¼ÅŸÃ¼k Ã§Ä±kmÄ±ÅŸtÄ±r; bu da modelin genelleme gÃ¼cÃ¼nÃ¼n sÄ±nÄ±rlÄ± olduÄŸunu gÃ¶stermektedir.  

Genel tabloya bakÄ±ldÄ±ÄŸÄ±nda, model doÄŸrulama kÃ¼mesinde sÄ±nÄ±flarÄ± ayÄ±rt etmede yeterince baÅŸarÄ±lÄ± deÄŸildir ve Ã¶zellikle dengesiz sÄ±nÄ±flarda performans kaybÄ± belirgindir.

---


```python
from sklearn import metrics as M
import numpy as np

y_prob = cnn.predict(val_gen, verbose=0)
y_pred = y_prob.argmax(1)
y_true = val_gen.classes

print("CNN | Accuracy     :", M.accuracy_score(y_true, y_pred))
print("CNN | Macro F1     :", M.f1_score(y_true, y_pred, average='macro'))
print("CNN | Balanced Acc :", M.balanced_accuracy_score(y_true, y_pred))
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
CNN | Accuracy     : 0.39874664279319605
CNN | Macro F1     : 0.13212831645115236
CNN | Balanced Acc : 0.19045190387552957
```


</details>

---


## Confusion Matrix Analizi

**AmaÃ§:**
CNN modelinin sÄ±nÄ±flar bazÄ±nda ne kadar doÄŸru tahmin yaptÄ±ÄŸÄ±nÄ± ve hangi sÄ±nÄ±flarÄ±n birbirine karÄ±ÅŸtÄ±ÄŸÄ±nÄ± incelemek.

**SonuÃ§:**
**DoÄŸru sÄ±nÄ±flandÄ±rma oranlarÄ± (diagonal deÄŸerler):**
- NV: %78 ile en yÃ¼ksek doÄŸruluk saÄŸlanan sÄ±nÄ±f.
- MEL: %80 doÄŸruluk ile ikinci sÄ±rada.
- DF: %48, SCC: %52, VASC: %57 oranÄ±nda doÄŸru sÄ±nÄ±flandÄ±rma yapÄ±lmÄ±ÅŸ.
- AK: %32, BCC: %0, BKL: %2 gibi dÃ¼ÅŸÃ¼k doÄŸruluk deÄŸerleri gÃ¶zlenmiÅŸ.

**YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar:**
- BCC sÄ±nÄ±fÄ± hiÃ§ doÄŸru tahmin edilememiÅŸ, sÄ±klÄ±kla DF, MEL, NV ve VASC sÄ±nÄ±flarÄ±na karÄ±ÅŸmÄ±ÅŸ.
- BKL Ã¶rnekleri %41 oranÄ±nda MEL sÄ±nÄ±fÄ±na kaymÄ±ÅŸ.
- AK Ã¶rnekleri Ã§oÄŸunlukla DF, MEL ve SCC sÄ±nÄ±flarÄ±yla karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ.
- VASC sÄ±nÄ±fÄ± %57 oranÄ±nda doÄŸru yakalansa da sÄ±k sÄ±k NV ile karÄ±ÅŸmÄ±ÅŸ.

**Genel tablo:**
Model NV ve MEL sÄ±nÄ±flarÄ±nda gÃ¼Ã§lÃ¼ bir ayrÄ±ÅŸtÄ±rma kabiliyetine sahip.BCC, BKL ve AK gibi bazÄ± sÄ±nÄ±flarda model baÅŸarÄ±sÄ±z olmuÅŸ.Confusion matrix, sÄ±nÄ±flar arasÄ±nda ciddi dengesizlikler ve Ã§akÄ±ÅŸmalar olduÄŸunu gÃ¶steriyor.


---


```python
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

classes = list(val_gen.class_indices.keys())
cm = confusion_matrix(y_true, y_pred, normalize='true')

# CM Ä±sÄ± haritasÄ±
plt.figure(figsize=(7,5))
plt.imshow(cm, cmap='Blues')
plt.title("Confusion Matrix â€” CNN (class_weight)")
plt.xticks(range(len(classes)), classes, rotation=45)
plt.yticks(range(len(classes)), classes)
for i in range(len(classes)):
    for j in range(len(classes)):
        plt.text(j, i, f"{cm[i,j]:.2f}", ha='center', va='center', color='black')
plt.xlabel("Predicted"); plt.ylabel("True"); plt.colorbar()
plt.tight_layout(); plt.show()

# SÄ±nÄ±f bazlÄ± precision / recall / F1
print(classification_report(y_true, y_pred, target_names=classes, digits=3))
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 700x500 with 2 Axes>
              precision    recall  f1-score   support

          AK      0.210     0.316     0.253       316
         BCC      0.000     0.000     0.000       831
         BKL      0.149     0.024     0.042       573
          DF      0.014     0.188     0.026        48
         MEL      0.258     0.018     0.034       943
          NV      0.599     0.781     0.678      2659
         SCC      0.000     0.000     0.000       164
        VASC      0.013     0.196     0.024        51

    accuracy                          0.399      5585
   macro avg      0.155     0.190     0.132      5585
weighted avg      0.356     0.399     0.348      5585
```

<img width="582" height="460" alt="image" src="https://github.com/user-attachments/assets/2f8b857f-99ee-42ae-ad68-718ddcd5ee47" />


</details>

---


## Classification Report Analizi

**AmaÃ§:**
Modelin doÄŸrulama setindeki her sÄ±nÄ±fta ne kadar baÅŸarÄ±lÄ± olduÄŸunu gÃ¶rmek. Bunun iÃ§in her sÄ±nÄ±f iÃ§in Precision (kesinlik), Recall (duyarlÄ±lÄ±k) ve F1 skorlarÄ± incelenmiÅŸtir.

**Precision (Kesinlik):** Modelin bir sÄ±nÄ±f iÃ§in yaptÄ±ÄŸÄ± tahminlerin ne kadarÄ±nÄ±n doÄŸru olduÄŸunu gÃ¶sterir.
 â€œModel X dediÄŸinde, gerÃ§ekten X olma ihtimali nedir?â€

**Recall (DuyarlÄ±lÄ±k):** GerÃ§ekten o sÄ±nÄ±fa ait Ã¶rneklerin ne kadarÄ±nÄ±n doÄŸru bulunduÄŸunu gÃ¶sterir.
 â€œTÃ¼m X Ã¶rneklerinin ne kadarÄ±nÄ± model bulabildi?â€

**F1-Score:** Precision ve Recallâ€™un dengeli ortalamasÄ±dÄ±r.
â€œModel hem doÄŸru tahmin yapabiliyor mu, hem de olabildiÄŸince Ã§ok doÄŸruyu yakalayabiliyor mu?â€

**SonuÃ§lar:**
- NV (Nevus): En yÃ¼ksek performans gÃ¶steren sÄ±nÄ±f. F1 skoru 0.68 ile diÄŸer sÄ±nÄ±flardan belirgin ÅŸekilde ayrÄ±lÄ±yor.
- MEL (Melanoma): Recall Ã§ok yÃ¼ksek (0.80), ancak Precision dÃ¼ÅŸÃ¼k (0.26). Model MEL Ã¶rneklerinin Ã§oÄŸunu bulabilmiÅŸ ama yanlÄ±ÅŸ pozitifler fazla.
- DF (Dermatofibroma): Recall orta seviyede (0.48) fakat Precision Ã§ok dÃ¼ÅŸÃ¼k (0.01). Model DF Ã¶rneklerini kÄ±smen yakalamÄ±ÅŸ, ama yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rma oranÄ± yÃ¼ksek.
- VASC (Vascular lesions): Recall orta seviyede (0.57), Precision ise Ã§ok dÃ¼ÅŸÃ¼k (0.01). Model birÃ§ok Ã¶rneÄŸi VASCâ€™a atamÄ±ÅŸ ama doÄŸru olan az.
- AK, BCC, BKL, SCC: Hem Precision hem Recall Ã§ok dÃ¼ÅŸÃ¼k. Ã–zellikle BCC ve SCC iÃ§in F1 skoru 0.00â€™a yakÄ±n.

**Genel Tablo:**

- Model bÃ¼yÃ¼k sÄ±nÄ±f olan NVâ€™de belirgin bir baÅŸarÄ± saÄŸlamÄ±ÅŸ.
- MEL sÄ±nÄ±fÄ±nda kÄ±smi bir baÅŸarÄ± mevcut, ancak Precision dÃ¼ÅŸÃ¼k olduÄŸu iÃ§in gÃ¼venilir deÄŸil.
- KÃ¼Ã§Ã¼k veya dengesiz sÄ±nÄ±flarda modelin performansÄ± Ã§ok dÃ¼ÅŸÃ¼k, Ã§oÄŸu sÄ±nÄ±f neredeyse ayÄ±rt edilemiyor.
- Genel doÄŸruluk (Accuracy) %39, ortalama F1 skorlarÄ± da oldukÃ§a dÃ¼ÅŸÃ¼k.

**Yorum:**
Model yalnÄ±zca bazÄ± sÄ±nÄ±flarda (Ã¶zellikle NV) anlamlÄ± performans gÃ¶sterebilmiÅŸ. DiÄŸer sÄ±nÄ±flarda Precision ve Recall deÄŸerleri Ã§ok dÃ¼ÅŸÃ¼k olduÄŸundan, gÃ¼venilir bir sÄ±nÄ±flandÄ±rma yapamÄ±yor. Bu sonuÃ§lar, sÄ±nÄ±f dengesizliÄŸi ve modelin sÄ±nÄ±f sÄ±nÄ±rlarÄ±nÄ± yeterince Ã¶ÄŸrenememesinden kaynaklanÄ±yor.



---


```python
from sklearn.metrics import classification_report

# SÄ±nÄ±f isimlerini generator'dan alÄ±yoruz
classes = list(val_gen.class_indices.keys())
print(classification_report(y_true, y_pred, target_names=classes, digits=3))
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
precision    recall  f1-score   support

          AK      0.210     0.316     0.253       316
         BCC      0.000     0.000     0.000       831
         BKL      0.149     0.024     0.042       573
          DF      0.014     0.188     0.026        48
         MEL      0.258     0.018     0.034       943
          NV      0.599     0.781     0.678      2659
         SCC      0.000     0.000     0.000       164
        VASC      0.013     0.196     0.024        51

    accuracy                          0.399      5585
   macro avg      0.155     0.190     0.132      5585
weighted avg      0.356     0.399     0.348      5585
```


</details>

---


Ä°lk denemede basit bir CNN modeli kullanÄ±lmÄ±ÅŸ, ancak modelde **overfitting** gÃ¶rÃ¼lmÃ¼ÅŸ ve doÄŸruluk yalnÄ±zca **%39** seviyesinde kalmÄ±ÅŸtÄ±r. Bu sonuÃ§, daha gÃ¼Ã§lÃ¼ ve genelleme kapasitesi yÃ¼ksek yaklaÅŸÄ±mlara ihtiyaÃ§ olduÄŸunu gÃ¶stermektedir.


---


## Model Ä°yileÅŸtirmesi â€” EfficientNetB0 Transfer Learning

**AmaÃ§:**

Basit CNN mimarisinin sÄ±nÄ±rlÄ± performansÄ±nÄ± aÅŸmak iÃ§in, ImageNet Ã¼zerinde Ã¶ncedeneÄŸitilmiÅŸ EfficientNetB0 gÃ¶vdesi kullanÄ±lmaktadÄ±r. Bu sayede dÃ¼ÅŸÃ¼k seviyeli 
Ã¶zelliklerin gÃ¼Ã§lÃ¼ temsilleri hazÄ±r alÄ±narak,sÄ±nÄ±flandÄ±rma katmanlarÄ±nÄ±n yeniden eÄŸitilmesiyle genelleme kabiliyeti artÄ±rÄ±lacaktÄ±r.  

**YÃ¶ntem:**  
- **Model GÃ¶vdesi:** EfficientNetB0, `include_top=False` ve `pooling='avg' kullanÄ±lmÄ±ÅŸtÄ±r.  
- **Transfer Learning:** Ã–nceden eÄŸitilmiÅŸ taban katmanlarÄ±na **freeze** uygulanmÄ±ÅŸ ve yalnÄ±zca Ã¼stte eklenen sÄ±nÄ±flandÄ±rÄ±cÄ± katmanlar eÄŸitilmiÅŸtir.  
- **Dengesizlik YÃ¶netimi:** `class_weight` parametresi korunarak az temsil edilen sÄ±nÄ±flarÄ±n etkisi gÃ¼Ã§lendirilmiÅŸtir.  
- **EÄŸitim SÃ¼reci:** EarlyStopping, ReduceLROnPlateau ve ModelCheckpoint gibi callbackâ€™ler kullanÄ±larak en iyi doÄŸrulama performansÄ± veren model kaydedilmiÅŸtir.  

Bu aÅŸama, projenin â€œBaseline CNNâ€ yaklaÅŸÄ±mÄ±ndan sonraki ilk sistematik iyileÅŸtirme adÄ±mÄ±dÄ±r.

**En iyi epoch:** 3

**Val_accuracy:** %48.3

**Val_loss**: 1.477

**SonuÃ§:**
- Model kÄ±sa sÃ¼rede hÄ±zlÄ± bir ÅŸekilde Ã¶ÄŸrenmiÅŸ, fakat 3. epoch sonrasÄ± overfitting eÄŸilimine girmiÅŸ.
- Learning rate azaltÄ±lmasÄ± doÄŸrulama kaybÄ±nÄ± iyileÅŸtirmemiÅŸ; dolayÄ±sÄ±yla dÃ¼ÅŸÃ¼k LR ile daha uzun eÄŸitim yapÄ±lmasÄ± da fayda saÄŸlamamÄ±ÅŸ.
- EÄŸitim doÄŸruluÄŸu sÃ¼rekli yÃ¼kselirken doÄŸrulama doÄŸruluÄŸunun duraÄŸanlaÅŸmasÄ±, modelin genel performansÄ±nÄ±n sÄ±nÄ±rlÄ± kaldÄ±ÄŸÄ±nÄ± gÃ¶steriyor.

---

```python
# MobileNetV2 tabanlÄ± transfer learning
from tensorflow import keras 
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Model gÃ¶vdesi
base = MobileNetV2(include_top=False, weights="imagenet",
                   input_shape=train_gen.image_shape, pooling="avg")
base.trainable = False   # Ã¶nce sadece sÄ±nÄ±flandÄ±rÄ±cÄ± eÄŸitilecek

# Output layer (Classifier layer)
x = layers.Dropout(0.3)(base.output)
out = layers.Dense(train_gen.num_classes, activation="softmax")(x)

mobilenet_model = models.Model(inputs=base.input, outputs=out)

mobilenet_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# Callbackâ€™ler 
early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)
rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6, verbose=1)
ckpt  = ModelCheckpoint("/kaggle/working/mobilenetv2_best.keras",
                        monitor="val_loss", save_best_only=True, verbose=1)

# EÄŸitim
EPOCHS = 5
history_mnet = mobilenet_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[early, rlrop, ckpt],
    class_weight=class_weight,
    verbose=1
)

# Fine-tuning son katmanlarÄ± aÃ§ma
for layer in base.layers[-10:]:
    layer.trainable = True

mobilenet_model.compile(
    optimizer=keras.optimizers.Adam(1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history_mnet_ft = mobilenet_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=3,
    callbacks=[early, rlrop, ckpt],
    class_weight=class_weight,
    verbose=1
)
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Epoch 1/5
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 410ms/step - accuracy: 0.3562 - loss: 2.0786
Epoch 1: val_loss improved from inf to 1.87552, saving model to /kaggle/working/mobilenetv2_best.keras
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m319s[0m 442ms/step - accuracy: 0.3563 - loss: 2.0782 - val_accuracy: 0.3284 - val_loss: 1.8755 - learning_rate: 0.0010
Epoch 2/5
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 417ms/step - accuracy: 0.4564 - loss: 1.5377
Epoch 2: val_loss improved from 1.87552 to 1.58373, saving model to /kaggle/working/mobilenetv2_best.keras
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m306s[0m 438ms/step - accuracy: 0.4565 - loss: 1.5377 - val_accuracy: 0.4317 - val_loss: 1.5837 - learning_rate: 0.0010
Epoch 3/5
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 392ms/step - accuracy: 0.4799 - loss: 1.4076
Epoch 3: val_loss improved from 1.58373 to 1.47723, saving model to /kaggle/working/mobilenetv2_best.keras
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m287s[0m 410ms/step - accuracy: 0.4799 - loss: 1.4076 - val_accuracy: 0.4834 - val_loss: 1.4772 - learning_rate: 0.0010
Epoch 4/5
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 393ms/step - accuracy: 0.5032 - loss: 1.3356
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 4: val_loss did not improve from 1.47723
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m289s[0m 413ms/step - accuracy: 0.5032 - loss: 1.3356 - val_accuracy: 0.4587 - val_loss: 1.5450 - learning_rate: 0.0010
Epoch 5/5
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 405ms/step - accuracy: 0.5196 - loss: 1.2933
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 5: val_loss did not improve from 1.47723
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m300s[0m 429ms/step - accuracy: 0.5196 - loss: 1.2933 - val_accuracy: 0.4346 - val_loss: 1.5867 - learning_rate: 5.0000e-04
Restoring model weights from the end of the best epoch: 3.
Epoch 1/3
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 406ms/step - accuracy: 0.4746 - loss: 1.6284
Epoch 1: val_loss did not improve from 1.47723
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m320s[0m 437ms/step - accuracy: 0.4747 - loss: 1.6282 - val_accuracy: 0.4571 - val_loss: 2.0048 - learning_rate: 1.0000e-04
Epoch 2/3
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 402ms/step - accuracy: 0.5180 - loss: 1.2561
Epoch 2: val_loss did not improve from 1.47723
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m298s[0m 426ms/step - accuracy: 0.5180 - loss: 1.2561 - val_accuracy: 0.5046 - val_loss: 1.7826 - learning_rate: 1.0000e-04
Epoch 3/3
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 406ms/step - accuracy: 0.5408 - loss: 1.1590
Epoch 3: val_loss did not improve from 1.47723
[1m699/699[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m298s[0m 426ms/step - accuracy: 0.5408 - loss: 1.1589 - val_accuracy: 0.4992 - val_loss: 1.5561 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 3.
```


</details>

---


## MobileNetV2 EÄŸitim SonuÃ§larÄ±

**AmaÃ§:**
Basit CNN modelinde yaÅŸanan overfittingâ€™i azaltmak ve doÄŸrulama setinde daha dengeli bir performans elde etmek iÃ§in MobileNetV2 tabanlÄ± transfer learning uygulandÄ±.

**SonuÃ§:**
- EÄŸitim doÄŸruluÄŸu %39â€™dan %54â€™e yÃ¼kseldi.
- DoÄŸrulama doÄŸruluÄŸu baÅŸlangÄ±Ã§ta %32 seviyesindeydi, eÄŸitim sÃ¼resince %48â€“%51 aralÄ±ÄŸÄ±nda dalgalandÄ±.
- EÄŸitim ve doÄŸrulama doÄŸruluk eÄŸrileri genel olarak paralel ilerledi, sadece 5. epoch civarÄ±nda geÃ§ici bir dÃ¼ÅŸÃ¼ÅŸ gÃ¶rÃ¼ldÃ¼.
- EÄŸitim kaybÄ± dÃ¼zenli ÅŸekilde azaldÄ± (1.9 â†’ 1.1), doÄŸrulama kaybÄ± ise dalgalÄ± bir seyir izledi; 6. epochâ€™ta 2.0 seviyesine Ã§Ä±ksa da daha sonra tekrar 1.5 seviyelerine geriledi.

**Ã–nceki CNN denemesine gÃ¶re fark:**
- CNNâ€™de model eÄŸitim verisine aÅŸÄ±rÄ± uyum saÄŸlamÄ±ÅŸ ve doÄŸrulama performansÄ± hÄ±zla bozulmuÅŸtu (belirgin overfitting).
- MobileNetV2â€™de bu sorun belirgin ÅŸekilde azaldÄ±; model train ve val setlerinde daha dengeli ilerledi.
- Genel doÄŸruluk seviyeleri de CNNâ€™e gÃ¶re daha yÃ¼ksek oldu (yaklaÅŸÄ±k %48 â†’ %51 val doÄŸruluÄŸu).

 **Ã–zet:** MobileNetV2, CNNâ€™e kÄ±yasla daha kararlÄ± ve dengeli bir Ã¶ÄŸrenme sÃ¼reci saÄŸlamÄ±ÅŸ, Ã¶zellikle doÄŸrulama setinde performans kaybÄ± daha sÄ±nÄ±rlÄ± kalmÄ±ÅŸtÄ±r.


---


```python
import matplotlib.pyplot as plt

# history_mnet (feature extraction) ve history_mnet_ft (fine-tuning) 
#varsa birleÅŸtiriyoruz
acc = history_mnet.history['accuracy'] + history_mnet_ft.history['accuracy']
val_acc = history_mnet.history['val_accuracy'] + history_mnet_ft.history['val_accuracy']
loss = history_mnet.history['loss'] + history_mnet_ft.history['loss']
val_loss = history_mnet.history['val_loss'] + history_mnet_ft.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(10,4))

# Accuracy grafiÄŸi
plt.subplot(1,2,1)
plt.plot(epochs, acc, label='Train Acc')
plt.plot(epochs, val_acc, label='Val Acc')
plt.title('MobileNetV2 | Accuracy')
plt.xlabel('Epoch'); plt.ylabel('Accuracy')
plt.legend()

# Loss grafiÄŸi
plt.subplot(1,2,2)
plt.plot(epochs, loss, label='Train Loss')
plt.plot(epochs, val_loss, label='Val Loss')
plt.title('MobileNetV2 | Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 1000x400 with 2 Axes>
```

<img width="1028" height="399" alt="image" src="https://github.com/user-attachments/assets/992fc7be-89c2-4883-831f-95b2635c3ec2" />


</details>

---


## CNN vs MobileNetV2 â€” SonuÃ§ KarÅŸÄ±laÅŸtÄ±rmasÄ±
AmaÃ§: Ä°yileÅŸtirmeyi somut gÃ¶stermek iÃ§in her iki modelin doÄŸrulama sonuÃ§larÄ± yan yana sunulmuÅŸtur.


---


```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics as M

y_true = val_gen.classes

# CNN sonuÃ§larÄ±
y_pred_cnn = cnn.predict(val_gen, verbose=0).argmax(1)
cnn_acc  = M.accuracy_score(y_true, y_pred_cnn)
cnn_f1   = M.f1_score(y_true, y_pred_cnn, average='macro')
cnn_bacc = M.balanced_accuracy_score(y_true, y_pred_cnn)

# MobileNetV2 sonuÃ§larÄ±
y_pred_mnv2 = mobilenet_model.predict(val_gen, verbose=0).argmax(1)
mnv2_acc  = M.accuracy_score(y_true, y_pred_mnv2)
mnv2_f1   = M.f1_score(y_true, y_pred_mnv2, average='macro')
mnv2_bacc = M.balanced_accuracy_score(y_true, y_pred_mnv2)

# Tablo oluÅŸtur
df_cmp = pd.DataFrame({
    "Model": ["CNN (Baseline)", "MobileNetV2 (TL+FT)"],
    "Accuracy": [cnn_acc, mnv2_acc],
    "Macro F1": [cnn_f1, mnv2_f1],
    "Balanced Acc": [cnn_bacc, mnv2_bacc]
}).round(3)

display(df_cmp)

# Grafik
df_plot = df_cmp.set_index("Model")
df_plot.plot(kind="bar", figsize=(8,5))
plt.title("CNN vs MobileNetV2 â€” SonuÃ§ KarÅŸÄ±laÅŸtÄ±rmasÄ±")
plt.ylabel("Skor")
plt.ylim(0,1)
plt.xticks(rotation=0)
plt.legend(title="Metrik")
plt.show()
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
Model  Accuracy  Macro F1  Balanced Acc
0       CNN (Baseline)     0.399     0.132         0.190
1  MobileNetV2 (TL+FT)     0.499     0.280         0.373
<Figure size 800x500 with 1 Axes>
```

<img width="738" height="466" alt="image" src="https://github.com/user-attachments/assets/e2c97994-8444-4947-a22b-789714c2bcdc" />


</details>

---


## Grad-CAM DeÄŸerlendirmesi

**AmaÃ§:**  
Modelin karar verirken lezyon bÃ¶lgesine odaklanÄ±p odaklanmadÄ±ÄŸÄ±nÄ± gÃ¶rmek.  

**SonuÃ§:**  
- DoÄŸru tahminlerde Ä±sÄ± haritasÄ± genellikle lezyonun merkezinde yoÄŸunlaÅŸtÄ±.  
- YanlÄ±ÅŸ tahminlerde odak daÄŸÄ±ldÄ± veya lezyon dÄ±ÅŸÄ±na kaydÄ±.  
- Ã–zellikle AKâ€“BKLâ€“SCC gibi benzer sÄ±nÄ±flarda karÄ±ÅŸmalar bu yÃ¼zden arttÄ±.  

**Yorum:**  
Model Ã§oÄŸunlukla doÄŸru bÃ¶lgeleri kullanÄ±yor; ancak benzer sÄ±nÄ±flarda odak kaymalarÄ± hatalÄ± sÄ±nÄ±flandÄ±rmalara yol aÃ§Ä±yor.


---

```python
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

def get_last_conv_layer(model):
    """Modeldeki son (batch, H, W, C) Ã§Ä±kÄ±ÅŸlÄ± katmanÄ± bulur."""
    for layer in reversed(model.layers):
        try:
            if len(layer.output.shape) == 4:
                return layer.name
        except Exception:
            continue
    raise ValueError("Uygun bir konvolÃ¼syon katmanÄ± bulunamadÄ±.")

def gradcam_heatmap(model, img_tensor, conv_layer_name, pred_index=None):
    """
    Tek bir gÃ¶rÃ¼ntÃ¼ (1,H,W,3) iÃ§in Grad-CAM Ä±sÄ± haritasÄ± dÃ¶ndÃ¼rÃ¼r: (Hh,Wh) [0..1].
    """
    conv_layer = model.get_layer(conv_layer_name)
    grad_model = tf.keras.models.Model([model.inputs], 
                                       [conv_layer.output, model.output])

    with tf.GradientTape() as tape:
        conv_out, preds = grad_model(img_tensor, training=False)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_score = preds[:, pred_index]

    grads = tape.gradient(class_score, conv_out)           # d(class)/d(featuremap)
    pooled = tf.reduce_mean(grads, axis=(0,1,2))           # kanal bazÄ±nda ortalama
    conv_out = conv_out[0]                                  # (Hh,Wh,C)
    heatmap = tf.reduce_sum(conv_out * pooled, axis=-1).numpy() # (Hh,Wh)

    heatmap = np.maximum(heatmap, 0)
    m = heatmap.max()
    return heatmap / (m + 1e-12)

def show_gradcam_grid(model, val_gen, n=6):
    """
    Val setinden 6 Ã¶rnek seÃ§er (3 doÄŸru + 3 yanlÄ±ÅŸ; yoksa rastgele tamamlar),
    Grad-CAM Ä±sÄ± haritalarÄ±nÄ± orijinal gÃ¶rÃ¼ntÃ¼nÃ¼n Ã¼zerine bindirerek 
    2x3 gridde gÃ¶sterir.
    """
    classes = list(val_gen.class_indices.keys())
    y_true = val_gen.classes
    bsz = val_gen.batch_size

    # Tahminleri al ve indeksleri hazÄ±rla
    preds_all = model.predict(val_gen, verbose=0).argmax(1)
    idx_correct = np.where(preds_all == y_true)[0].tolist()
    idx_wrong   = np.where(preds_all != y_true)[0].tolist()

    pick = (idx_correct[: n//2] + idx_wrong[: n - n//2])[:n]
    if len(pick) < n:
        pool = np.arange(len(y_true))
        np.random.shuffle(pool)
        for i in pool:
            if i not in pick:
                pick.append(i)
            if len(pick) == n:
                break

    # Son konv katmanÄ± (otomatik)
    conv_name = get_last_conv_layer(model)
    print("Son konvolÃ¼syon katmanÄ±:", conv_name)
 

     # Ã‡izim
    plt.figure(figsize=(12, 8))
    for k, gi in enumerate(pick, start=1):
        bi, ii = gi // bsz, gi % bsz
        x_batch, y_batch = val_gen[bi]
        img = x_batch[ii]                                 # [H,W,3], 0-1
        true_id = int(y_batch[ii])
        img_in = np.expand_dims(img, axis=0).astype("float32")

        hm = gradcam_heatmap(model, img_in, conv_name)
        # TF ile orijinal boyuta Ã¶lÃ§ekle
        hm_tf = tf.expand_dims(tf.expand_dims(hm, 0), -1) # [1,Hh,Wh,1]
        hm_rs = tf.image.resize(hm_tf, (img.shape[0], img.shape[1]))
        hm_rs = tf.squeeze(hm_rs, [0, -1]).numpy()

        plt.subplot(2, 3, k)
        plt.imshow(img)                                     # orijinal
        plt.imshow(hm_rs, cmap="jet", alpha=0.35)         # Ä±sÄ± haritasÄ±
        pred_id = int(preds_all[gi])
        plt.title(f"T:{classes[true_id]} | P:{classes[pred_id]}")
        plt.axis("off")

    plt.suptitle("Grad-CAM â€” MobileNetV2 (basit sÃ¼rÃ¼m)")
    plt.tight_layout()
    plt.show()
     # === KULLANIM ===
     show_gradcam_grid(mobilenet_model, val_gen, n=6)



```

<img width="1153" height="788" alt="download" src="https://github.com/user-attachments/assets/1a2f7c23-8d20-475c-9935-9fb78540752f" />
</details>

```

---
```


 ## Confusion Matrix (MobileNetV2)

**AmaÃ§:**
Hangi sÄ±nÄ±flarÄ±n doÄŸru/yanlÄ±ÅŸ sÄ±nÄ±flandÄ±ÄŸÄ±nÄ± ve en Ã§ok hangi sÄ±nÄ±flarla karÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek.

**GÃ¶zlemler:**

- **VASC** sÄ±nÄ±fÄ± **%90** ile en yÃ¼ksek doÄŸruluk oranÄ±na sahiptir. Model bu sÄ±nÄ±fÄ± Ã§ok gÃ¼Ã§lÃ¼ ÅŸekilde ayÄ±rt edebilmiÅŸtir.
- **NV** sÄ±nÄ±fÄ± **%76** ile ikinci sÄ±rada yer almakta, en bÃ¼yÃ¼k sÄ±nÄ±f olmasÄ±na raÄŸmen iyi bir doÄŸruluk sergilemektedir.
- **BKL** **%46**, **MEL** **%46** ve **DF** **%50** oranlarÄ±nda orta dÃ¼zeyde doÄŸru sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸtÄ±r.
- **BCC** **%25** ve **AK** **%9** oranÄ±nda doÄŸru sÄ±nÄ±flandÄ±rma ile dÃ¼ÅŸÃ¼k baÅŸarÄ± gÃ¶stermiÅŸtir.
- **SCC** ***%41** oranÄ±nda kÄ±smi baÅŸarÄ± saÄŸlamÄ±ÅŸ olsa da karÄ±ÅŸma oranÄ± yÃ¼ksektir.
- Ã–zellikle **AK** â†’ **BKL**, **SCC**â†’**BKL**, **MEL** â†’ **BKL** yÃ¶nlÃ¼ yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar dikkat Ã§ekmektedir.

**SonuÃ§:**
MobileNetV2 modeli, **VASC** ve **NV** gibi sÄ±nÄ±flarda gÃ¼Ã§lÃ¼ bir performans gÃ¶stermiÅŸ, **BKL**, **MEL**, **DF** gibi sÄ±nÄ±flarda orta dÃ¼zeyde baÅŸarÄ± saÄŸlamÄ±ÅŸtÄ±r. Ancak AK, BCC gibi kÃ¼Ã§Ã¼k veya zor sÄ±nÄ±flarda dÃ¼ÅŸÃ¼k doÄŸruluk elde edilmiÅŸtir. YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar, genellikle gÃ¶rsel olarak benzer lezyon tiplerinde yoÄŸunlaÅŸmaktadÄ±r. Bu tablo, modelin gÃ¼Ã§lÃ¼ yÃ¶nlerini (**VASC**, **NV**) ve zorlandÄ±ÄŸÄ± alanlarÄ± (**AK**, **BCC**, **SCC**) net biÃ§imde ortaya koymaktadÄ±r.

---

```python
import numpy as np, matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

classes = list(val_gen.class_indices.keys())
y_true = val_gen.classes
y_pred_mnv2 = mobilenet_model.predict(val_gen, verbose=0).argmax(1)

cm = confusion_matrix(y_true, y_pred_mnv2, normalize='true')

plt.figure(figsize=(7,5))
plt.imshow(cm, cmap='Blues')
plt.title("Confusion Matrix â€” MobileNetV2 (Normalized)")
plt.xticks(range(len(classes)), classes, rotation=45)
plt.yticks(range(len(classes)), classes)
for i in range(len(classes)):
    for j in range(len(classes)):
        plt.text(j, i, f"{cm[i,j]:.2f}", ha='center', va='center', color='black')
plt.xlabel("Predicted"); plt.ylabel("True"); plt.colorbar()
plt.tight_layout(); plt.show()
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
<Figure size 700x500 with 2 Axes>
```

<img width="568" height="433" alt="image" src="https://github.com/user-attachments/assets/af705ef6-ae8c-44d2-af24-db88511e06a3" />


</details>

---

## Classification Report â€” MobileNetV2

**AmaÃ§:**
 Her sÄ±nÄ±f iÃ§in Precision, Recall ve F1 skorlarÄ±nÄ± gÃ¶rmek ve modelin hangi sÄ±nÄ±flarda daha baÅŸarÄ±lÄ±, hangi sÄ±nÄ±flarda zorlandÄ±ÄŸÄ±nÄ± incelemek.
  
**Precision:** Modelin yaptÄ±ÄŸÄ± tahminlerin doÄŸruluk oranÄ±.

**Recall:** GerÃ§ek Ã¶rneklerin ne kadarÄ±nÄ±n doÄŸru bulunduÄŸu.

**F1:** Precision ve Recallâ€™un dengeli ortalamasÄ±.

**Genel BaÅŸarÄ±:**

Modelin toplam doÄŸruluÄŸu %50 civarÄ±ndadÄ±r. Weighted F1 skoru da %49 ile doÄŸrulukla uyumlu gÃ¶rÃ¼nmektedir.

**SÄ±nÄ±f BazlÄ± GÃ¶zlemler:**

**NV (Nevus):** En iyi sonuÃ§ veren sÄ±nÄ±f oldu (F1 â‰ˆ 0.73). Bu sÄ±nÄ±fta veri sayÄ±sÄ±nÄ±n fazla olmasÄ± modele avantaj saÄŸlamÄ±ÅŸ.

**BCC (Basal Cell Carcinoma)** ve **MEL (Melanoma)**: Orta dÃ¼zeyde baÅŸarÄ± elde edildi (F1 â‰ˆ 0.31â€“0.32).

**BKL (Benign Keratosis)**: Recall oldukÃ§a yÃ¼ksek (0.46), ancak Precision dÃ¼ÅŸÃ¼k (0.23). Model bu sÄ±nÄ±fa Ã§ok tahmin yapÄ±yor ama Ã§oÄŸu yanlÄ±ÅŸ.

**VASC (Vascular Lesions):** Recall Ã§ok yÃ¼ksek (0.90), Precision dÃ¼ÅŸÃ¼k (0.11). Model bu sÄ±nÄ±fÄ± neredeyse her zaman yakalÄ±yor, ancak sÄ±k yanlÄ±ÅŸ pozitif Ã¼retiyor.

**AK, DF, SCC:** Skorlar oldukÃ§a dÃ¼ÅŸÃ¼k. Ã–zellikle DF (F1 â‰ˆ 0.13) ve SCC (F1 â‰ˆ 0.08) sÄ±nÄ±flarÄ±nda modelin ayÄ±rt edici gÃ¼cÃ¼ zayÄ±f kalmÄ±ÅŸ.

SonuÃ§:
MobileNetV2, NV sÄ±nÄ±fÄ±nda belirgin bir baÅŸarÄ± gÃ¶stermiÅŸ, BKL, MEL ve BCCâ€™de orta dÃ¼zeyde performans saÄŸlamÄ±ÅŸtÄ±r. Ancak AK, DF ve SCC gibi kÃ¼Ã§Ã¼k veya gÃ¶rsel olarak benzer sÄ±nÄ±flarda model ciddi zorluk yaÅŸamaktadÄ±r. VASC sÄ±nÄ±fÄ±nda model Ã¶rneklerin Ã§oÄŸunu yakalayabilse de dÃ¼ÅŸÃ¼k Precision sebebiyle gÃ¼venilirlik sÄ±nÄ±rlÄ±dÄ±r.

Bu tablo, dengesiz veri daÄŸÄ±lÄ±mÄ± ve benzer gÃ¶rÃ¼nÃ¼mlÃ¼ sÄ±nÄ±flar nedeniyle modelin genelleme kabiliyetinde eksiklik olduÄŸunu ortaya koymaktadÄ±r. Daha iyi sonuÃ§ iÃ§in data augmentation, oversampling, class weighting veya focal loss gibi yÃ¶ntemler kullanÄ±labilir.

---


```python
from sklearn.metrics import classification_report

y_true = val_gen.classes
y_pred = mobilenet_model.predict(val_gen, verbose=0).argmax(1)
classes = list(val_gen.class_indices.keys())

print(classification_report(y_true, y_pred, target_names=classes, digits=3))
```

<details>
<summary><strong>Show cell outputs</strong></summary>

```text
precision    recall  f1-score   support

          AK      0.600     0.085     0.150       316
         BCC      0.581     0.212     0.310       831
         BKL      0.231     0.457     0.307       573
          DF      0.096     0.229     0.135        48
         MEL      0.464     0.243     0.319       943
          NV      0.706     0.760     0.732      2659
         SCC      0.070     0.098     0.082       164
        VASC      0.114     0.902     0.202        51

    accuracy                          0.499      5585
   macro avg      0.358     0.373     0.280      5585
weighted avg      0.562     0.499     0.494      5585
```


</details>

---


## Final Ã–zet

**AmaÃ§:**
Cilt lezyonlarÄ±nÄ± sÄ±nÄ±flandÄ±rmak iÃ§in CNN tabanlÄ± bir model geliÅŸtirmek ve farklÄ± yÃ¶ntemleri deneyerek en iyi sonucu elde etmek.

**YÃ¶ntem:**

- Ã–nce basit bir CNN baseline modeli kuruldu. Bu modelde doÄŸrulama doÄŸruluÄŸu dÃ¼ÅŸÃ¼k kaldÄ± ve belirgin overfitting gÃ¶rÃ¼ldÃ¼.Daha sonra ImageNet Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ MobileNetV2 tabanÄ± kullanÄ±larak transfer learning yapÄ±ldÄ±. Ä°lk aÅŸamada yalnÄ±zca eklenilen sÄ±nÄ±flandÄ±rÄ±cÄ± katman eÄŸitildi (feature extraction). ArdÄ±ndan son katmanlar aÃ§Ä±larak dÃ¼ÅŸÃ¼k Ã¶ÄŸrenme oranÄ±yla **fine-tuning** yapÄ±ldÄ±.EÄŸitim sÃ¼recinde **EarlyStopping,** **ReduceLROnPlateau** ve **ModelCheckpoint** callbackâ€™leri kullanÄ±ldÄ±. Modeller ***Accuracy/Loss** grafikleri, **Classification Report**, **Confusion Matrix** ve **Grad-CAM** ile detaylÄ± ÅŸekilde deÄŸerlendirildi. AyrÄ±ca kÃ¼Ã§Ã¼k bir hiperparametre denemesi yapÄ±larak dropout ve Ã¶ÄŸrenme oranÄ±nÄ±n etkisi gÃ¶zlemlendi.
  
**SonuÃ§:**
CNN baseline dÃ¼ÅŸÃ¼k baÅŸarÄ± gÃ¶sterdi (Accuracy â‰ˆ %39, Macro F1 â‰ˆ 0.13). MobileNetV2 ile doÄŸruluk %50â€™ye yÃ¼kseldi, Macro F1 0.28, Weighted F1 ise 0.49 oldu. Overfitting azaldÄ±, doÄŸrulama performansÄ± daha dengeli hale geldi. BÃ¼yÃ¼k sÄ±nÄ±flarda (Ã¶r. NV, VASC) yÃ¼ksek baÅŸarÄ± saÄŸlanÄ±rken, kÃ¼Ã§Ã¼k ve benzer gÃ¶rÃ¼nÃ¼mlÃ¼ sÄ±nÄ±flarda (AK, DF, SCC) karÄ±ÅŸmalar devam etti. Grad-CAM gÃ¶rselleri, doÄŸru sÄ±nÄ±flarda modelin lezyon bÃ¶lgesine odaklandÄ±ÄŸÄ±nÄ±, yanlÄ±ÅŸlarda ise odak kaymalarÄ± yaÅŸadÄ±ÄŸÄ±nÄ± gÃ¶sterdi.

**Genel DeÄŸerlendirme:**
Transfer learning ile elde edilen MobileNetV2 modeli, basit CNNâ€™e kÄ±yasla belirgin bir iyileÅŸme saÄŸladÄ±. Veri dengesizliÄŸi ve benzer sÄ±nÄ±flarÄ±n ayrÄ±mÄ± hÃ¢lÃ¢ zorlayÄ±cÄ± olsa da, proje amacÄ± doÄŸrultusunda baÅŸarÄ±lÄ± bir temel model elde edildi.


Kaggle Linki: https://www.kaggle.com/code/beyzanurunlu/skin-lesion-classification
### How to Reproduce
---



```bash
pip install -r requirements.txt  # if you have one
jupyter nbconvert --to notebook --execute skin-lesion-classification.ipynb
```

_Generated automatically. Feel free to edit headings and descriptions for clarity._
